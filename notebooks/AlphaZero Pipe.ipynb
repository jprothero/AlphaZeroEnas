{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so what do we want\n",
    "#we want to utilize batching of neural network inputs (different from normal MCTS, so we cant just blindly follow\n",
    "#the example of the other paper)\n",
    "#and we also would like to maximize CPU and GPU resource utilization\n",
    "\n",
    "#so the pipeline thing does seem to be one of the best solutions for maximizing resource utilization\n",
    "#but for batching, I'm not totally sure.\n",
    "\n",
    "#so what would be the flow? we should write some pseudo code\n",
    "#for _ in number of sims -> send curr node?\n",
    "#otherwise we would always set the curr node to be a copy of the root node\n",
    "#thats probably okay\n",
    "\n",
    "#so basically we send some _ input into select, it doesnt matter, \n",
    "# so for task in range(num_sims), None\n",
    "#pipe.put(task)\n",
    "\n",
    "#then we need to specify the linking\n",
    "#basically what we want:\n",
    "#select(_) -> evaluate() -> expand(policy, value) -> backup(value)\n",
    "#so that's pretty simple, but the tricky part will be how to utilize batching with this\n",
    "#what we would need to do would be somehow to accumulate in evaluate, and then send it off to a simple queue\n",
    "#or something, and wait for the policy and value to return, so we would have normal multi processing queueing\n",
    "#inside of the pipe\n",
    "\n",
    "#that doesn't sound impossible by any means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T17:39:09.381946Z",
     "start_time": "2018-04-27T17:39:09.199719Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ipdb import set_trace\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "class AlphaZero:\n",
    "    def __init__(self, controller, turns_until_tau0=15, alpha=None, epsilon=.25, c=1):\n",
    "        self.turns_until_tau0 = turns_until_tau0\n",
    "        if alpha is None:\n",
    "            alpha = min(10/len(policy), 1)\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.c = c\n",
    "        \n",
    "        self.max_depth = controller.max_depth\n",
    "\n",
    "        self.root_node = {\n",
    "            \"children\": None,\n",
    "            \"parent\": None,\n",
    "            \"N\": 0,\n",
    "            \"d\": 0\n",
    "        }\n",
    "        \n",
    "        self.controller = controller\n",
    "        self.starting_indices = starting_indices\n",
    "        self.decision_list\n",
    "\n",
    "        self.turn = 1\n",
    "\n",
    "        self.T = 1\n",
    "\n",
    "    @staticmethod\n",
    "    def select(_):\n",
    "        node = self.root_node\n",
    "        \n",
    "        trajectory = []\n",
    "        while node[\"children\"] is not None and node[\"d\"] < self.max_depth:\n",
    "            choice_idx = node[\"max_uct_idx\"]\n",
    "            node[\"max_uct_idx\"] [\"children\"][choice_idx]\n",
    "            node = node[\"children\"][choice_idx]\n",
    "            node[\"max_uct\"]\n",
    "\n",
    "            d = node[\"d\"]\n",
    "            decision_idx = d % len(decision_list)\n",
    "            starting_idx = starting_indices[decision_idx]\n",
    "            emb_idx = starting_idx + choice_idx\n",
    "\n",
    "            trajectory.append(emb_idx)\n",
    "\n",
    "        return trajectory\n",
    "\n",
    "    def select_real(self, node, stochastic=True):\n",
    "        visits = np.array([child[\"N\"] for child in node[\"children\"]])\n",
    "\n",
    "        if self.T != 0:\n",
    "            visits_sum = (1.0 * visits.sum())\n",
    "\n",
    "            if visits_sum == 0:\n",
    "                set_trace()\n",
    "\n",
    "            assert visits_sum != 0\n",
    "            \n",
    "            visits = visits / visits_sum\n",
    "            idx = np.random.choice(len(visits), p=visits)\n",
    "        else:\n",
    "            idx = np.argmax(visits)\n",
    "\n",
    "        # if self.turn == self.turns_until_tau0:\n",
    "        #     self.T = 0\n",
    "\n",
    "        self.turn += 1\n",
    "\n",
    "        node = node[\"children\"][idx]\n",
    "        node[\"parent\"] = None\n",
    "\n",
    "        return idx, visits, node\n",
    "\n",
    "    @staticmethod\n",
    "    def get_policy_and_value(trajectory):\n",
    "        indices = []\n",
    "        weights = dict()\n",
    "        seen = dict()\n",
    "\n",
    "        scaled_one = 1/len(trajectory)\n",
    "        for emb_idx in trajectory:\n",
    "            try:\n",
    "                seen[emb_idx]\n",
    "            except:\n",
    "                seen[emb_idx] = True\n",
    "\n",
    "                try:\n",
    "                    weights[emb_idx] += scaled_one\n",
    "                except:\n",
    "                    weights[emb_idx] = scaled_one\n",
    "\n",
    "                indices.append(emb_idx)\n",
    "        \n",
    "        logits = []\n",
    "        embeddings = []\n",
    "        weights_list = []\n",
    "        for _, idx in enumerate(indices):\n",
    "            embeddings.append(self.embeddings[idx].unsqueeze(0))\n",
    "            logits.append(self.emb_merge_pre_softmax(self.embeddings[idx])[indices].unsqueeze(0))\n",
    "            weights_list.append(weights[idx])\n",
    "        weights = torch.from_numpy(np.array(weights_list)).float().unsqueeze(-1)\n",
    "        weights = weights.view(1, -1)\n",
    "        logits = torch.cat(logits)\n",
    "\n",
    "        probas = F.softmax(logits)\n",
    "        probas = weights.mm(probas)\n",
    "\n",
    "        probas2 = F.softmax(logits)*weights\n",
    "        probas2 = probas2.sum(0)\n",
    "\n",
    "        assert (probas == probas2).all()\n",
    "\n",
    "        probas /= probas.sum() #normalize\n",
    "        probas = probas.view(1, -1)\n",
    "        embeddings = torch.cat(embeddings)\n",
    "        final_emb = probas.mm(embeddings)\n",
    "\n",
    "        final_emb = final_emb.view(1, 1, -1)\n",
    "\n",
    "        cont_out = self.controller(final_emb)\n",
    "        \n",
    "        logits = self.softmaxs[decision_idx](cont_out).squeeze()\n",
    "        probas = F.softmax(logits)\n",
    "\n",
    "        return cont_out\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate(node, trajectory):\n",
    "        depth = node[\"d\"]\n",
    "        layer_idx = depth // len(self.decision_list)\n",
    "        decision_idx = depth % len(self.decision_list)\n",
    "        decision_name = self.decision_list[decision_idx]\n",
    "\n",
    "        while True:\n",
    "            skip_curr = self.check_condition(az, layer_idx, decision_name)\n",
    "            if not skip_curr:\n",
    "                break\n",
    "            else:\n",
    "                az.curr_node[\"d\"] += 1\n",
    "                depth = az.curr_node[\"d\"]\n",
    "                layer_idx = depth // len(self.decision_list)\n",
    "                decision_idx = depth % len(self.decision_list)\n",
    "                decision_name = self.decision_list[decision_idx]\n",
    "\n",
    "        if len(trajectory) > 0:\n",
    "            cont_out = get_policy_and_value(trajectory)\n",
    "\n",
    "        logits = self.softmaxs[decision_idx](cont_out).squeeze()\n",
    "        probas = F.softmax(logits)\n",
    "\n",
    "        probas_np = probas.detach().data.numpy()\n",
    "        # for name, arr in self.decisions.items():\n",
    "        #     if decision_name is name:\n",
    "        #         assert len(probas_np) == len(arr)\n",
    "\n",
    "        if self.mask_conditions[decision_name] is not None:\n",
    "            probas_np = self.mask_conditions[decision_name](layer_idx, probas_np)\n",
    "\n",
    "\n",
    "    #we should try to calculate as muchas possible outside this class\n",
    "    #so lets try to only pass what we need\n",
    "    def expand(self, policy):\n",
    "        self.curr_node[\"children\"] = []\n",
    "\n",
    "        if self.curr_node[\"parent\"] is None:\n",
    "            policy = self.add_dirichlet_noise(policy)\n",
    "\n",
    "        for p in policy:\n",
    "            child = {\n",
    "                \"N\": 0,\n",
    "                \"W\": 0,\n",
    "                \"Q\": 0,\n",
    "                \"U\": p,\n",
    "                \"P\": p,\n",
    "                \"d\": self.curr_node[\"d\"]+1,\n",
    "                \"children\": None,\n",
    "                \"parent\": self.curr_node\n",
    "            }\n",
    "\n",
    "            self.curr_node[\"children\"].extend([child])\n",
    "\n",
    "        return self.curr_node\n",
    "\n",
    "    def update_uct(self, node):\n",
    "        node[\"children_ucts\"] = []\n",
    "        for i, child in enumerate(node[\"children\"]):\n",
    "            child[\"U\"] = self.c*child[\"P\"] * \\\n",
    "                (1 + np.log(node[\"N\"])/(1 + child[\"N\"]))\n",
    "            child[\"UCT\"] = child[\"Q\"] + child[\"U\"]\n",
    "            node[\"children_ucts\"].append(child[\"UCT\"])\n",
    "            \n",
    "        node[\"children_ucts\"] = np.array(node[\"children_ucts\"])\n",
    "        \n",
    "        return node\n",
    "\n",
    "    @staticmethod\n",
    "    def backup(node, value):\n",
    "        value += 1\n",
    "        value /= 2\n",
    "        while node[\"parent\"] is not None:\n",
    "            node = self.update_node(node, value)\n",
    "\n",
    "            node = self.update_uct(node)\n",
    "            \n",
    "            node = node[\"parent\"]\n",
    "\n",
    "        node[\"N\"] += 1\n",
    "\n",
    "        node = self.update_uct(node)\n",
    "        \n",
    "        return node\n",
    "\n",
    "    def update_node(self, node, value):\n",
    "        node[\"N\"] += 1\n",
    "        node[\"W\"] += value\n",
    "        node[\"Q\"] = node[\"W\"]/node[\"N\"]\n",
    "        \n",
    "        return node\n",
    "\n",
    "    def add_dirichlet_noise(self, policy):\n",
    "        alpha = self.alpha \n",
    "        epsilon = self.epsilon\n",
    "        epsilon = 0.25\n",
    "        nu = np.random.dirichlet([alpha] * len(policy))\n",
    "        policy = policy*(1-epsilon) + nu*epsilon\n",
    "\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T17:39:21.612733Z",
     "start_time": "2018-04-27T17:39:21.604143Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpipe import OrderedStage, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T17:39:56.079649Z",
     "start_time": "2018-04-27T17:39:56.069134Z"
    }
   },
   "outputs": [],
   "source": [
    "az = AlphaZero(max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = az.select\n",
    "backup = az.backup\n",
    "up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1 = OrderedStage(select)\n",
    "# s2 = OrderedStage(evaluate)\n",
    "# p = Pipeline(s1.link(s2))\n",
    "\n",
    "# for task in 1, 2, 3, 4, 5, None:\n",
    "#     p.put(task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
